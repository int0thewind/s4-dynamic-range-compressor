{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyloudnorm as pyln\n",
    "import torch\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import freqz\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from s4drc.src.dataset import SignalTrainDatasetModule\n",
    "from s4drc.src.loss import FrechetAudioDistance, forge_validation_criterions_by\n",
    "from s4drc.src.model import S4Model\n",
    "from s4drc.src.module.db import convert_to_decibel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains routine to evaluate the model.\n",
    "\n",
    "Each individual evaluation task is wrapped in a function to prevent variables going global,\n",
    "All functions will save results to the local file system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparatory Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('./pl-experiment-result/')\n",
    "\n",
    "all_model_dirs = []\n",
    "for run_dir in sorted(ROOT_DIR.iterdir()):\n",
    "    if not run_dir.is_dir():\n",
    "        continue\n",
    "    with open(run_dir / 'checkpoints' / 'best-ckpt.txt', 'r') as f:\n",
    "        best_model_name = f.read().strip()\n",
    "    model_dir = run_dir / 'checkpoints' / best_model_name\n",
    "    assert model_dir.exists() and model_dir.is_file()\n",
    "    all_model_dirs.append(model_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ckpt_dir: Path):\n",
    "    return S4Model.load_from_checkpoint(ckpt_dir, map_location=device).eval()\n",
    "\n",
    "def get_testing_dataloader(ckpt_dir: Path, batch_size: int, testing_segment_length: int):\n",
    "    data_module = SignalTrainDatasetModule.load_from_checkpoint(ckpt_dir, map_location=device, batch_size=batch_size, testing_segment_length=testing_segment_length)\n",
    "    data_module.prepare_data()\n",
    "    return data_module.test_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All loss value are calculated using 2 ** 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    batch_size = 1\n",
    "    testing_dataloader = get_testing_dataloader(all_model_dirs[0], batch_size, 2 ** 23)  # match Steinmetz and Reiss\n",
    "\n",
    "    for model_dir in all_model_dirs:\n",
    "        model = get_model(model_dir).to(device)\n",
    "        validation_losses = defaultdict(float)\n",
    "\n",
    "        validation_criterions = forge_validation_criterions_by(model.hparams['loss_filter_coef']).to(device)\n",
    "        lufs_meter = pyln.Meter(SignalTrainDatasetModule.sample_rate)\n",
    "        fad = FrechetAudioDistance()\n",
    "\n",
    "        # The following saving tensors for \n",
    "        all_y = np.empty([len(testing_dataloader), 2 ** 23])\n",
    "        all_y_hat = np.empty([len(testing_dataloader), 2 ** 23])\n",
    "\n",
    "        for i, (x, y, parameters) in tqdm(\n",
    "            enumerate(testing_dataloader),\n",
    "            desc=f'Testing {model_dir}.',\n",
    "            total=len(testing_dataloader)\n",
    "        ):\n",
    "            x: Tensor = x.to(device)\n",
    "            y: Tensor = y.to(device)\n",
    "            parameters: Tensor = parameters.to(device)\n",
    "\n",
    "            y_hat: Tensor = model(x, parameters)\n",
    "\n",
    "            all_y[i, :] = y.flatten().cpu().numpy()\n",
    "            all_y_hat[i, :] = y_hat.flatten().cpu().numpy()\n",
    "\n",
    "            for validation_loss, validation_criterion in validation_criterions.items():\n",
    "                loss: Tensor = validation_criterion(y_hat.unsqueeze(1), y.unsqueeze(1))\n",
    "                validation_losses[validation_loss] += loss.item()\n",
    "\n",
    "            validation_losses['LUFS'] += np.abs(\n",
    "                lufs_meter.integrated_loudness(y.flatten().cpu().numpy()) -\n",
    "                lufs_meter.integrated_loudness(y_hat.flatten().cpu().numpy())\n",
    "            )\n",
    "        \n",
    "        for k, v in list(validation_losses.items()):\n",
    "            validation_losses[k] = v / len(testing_dataloader)\n",
    "        \n",
    "        # FAD should not be taken mean values\n",
    "        validation_losses['FAD'] = fad.score(all_y, all_y_hat)\n",
    "\n",
    "        validation_losses = {\n",
    "            k: [v] for k, v in validation_losses.items()\n",
    "        }\n",
    "    \n",
    "        pd.DataFrame(validation_losses).to_csv(model_dir.parent.parent / f'loss.csv')\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Direct Inference Efficiency (CUDA and CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_inference_efficiency():\n",
    "    for model_dir in all_model_dirs:\n",
    "        print(f'Calculating {model_dir} model inference efficiency.')\n",
    "        model = get_model(model_dir)\n",
    "        sample_lengths: list[int] = [2 ** i for i in range(5, 20)]\n",
    "\n",
    "        local_devices = [torch.device('cpu')]\n",
    "        if torch.cuda.is_available():\n",
    "            local_devices.append(torch.device('cuda'))\n",
    "\n",
    "        real_time_ratio_dict = defaultdict(list)\n",
    "        for local_device in local_devices:\n",
    "            if local_device.type == 'cpu':\n",
    "                print(f'Doing inference speed test on {(device_name := local_device.type).upper()}...')\n",
    "            elif local_device.type == 'cuda':\n",
    "                print(f'Doing inference speed test on {(device_name := torch.cuda.get_device_name())}.')\n",
    "            else:\n",
    "                raise NotImplementedError(f'Inference efficiency test can only run on CPU/CUDA')\n",
    "\n",
    "            model = model.to(local_device)\n",
    "\n",
    "            real_time_ratio_dict['device-name'].append(device_name)\n",
    "            \n",
    "            speed_ratios = []\n",
    "\n",
    "            for dataset_sample_length in sample_lengths:\n",
    "                dataset_sample_time_ns = dataset_sample_length * 1e9 / SignalTrainDatasetModule.sample_rate\n",
    "\n",
    "                inference_time_ns: list[int] = []\n",
    "                for _ in range(20):\n",
    "                    x = torch.rand(1, dataset_sample_length).to(local_device, torch.float32)\n",
    "                    cond = torch.tensor([[1, 65]]).to(local_device, torch.float32)\n",
    "\n",
    "                    tic = time.perf_counter_ns()\n",
    "                    model(x, cond)\n",
    "                    toc = time.perf_counter_ns()\n",
    "                    inference_time_ns.append(toc - tic)\n",
    "                \n",
    "                inference_time_ns_mean = mean(inference_time_ns)\n",
    "                real_time_ratio = dataset_sample_time_ns / inference_time_ns_mean\n",
    "                speed_ratios.append(real_time_ratio)\n",
    "                real_time_ratio_dict[str(dataset_sample_length)].append(real_time_ratio)\n",
    "        \n",
    "        pd.DataFrame(real_time_ratio_dict).to_csv(model_dir.parent.parent / 'efficiency.csv')\n",
    "\n",
    "\n",
    "evaluate_inference_efficiency()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output Audio (CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_output_audio():\n",
    "    testing_dataloader = get_testing_dataloader(all_model_dirs[0], 16, 10 * SignalTrainDatasetModule.sample_rate)\n",
    "\n",
    "    for model_dir in all_model_dirs:\n",
    "        print(f'Generating {model_dir} model output audio.')\n",
    "        model = get_model(model_dir).to(device)\n",
    "\n",
    "        output_audio_dir = model_dir.parent.parent / f'output-audio-10s'\n",
    "        output_audio_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        ii = 0\n",
    "        for x, y, cond in tqdm(testing_dataloader, desc='Generating output audio', total=len(testing_dataloader)):\n",
    "            x: Tensor = x.to(device)\n",
    "            y: Tensor = y.to(device)\n",
    "            cond: Tensor = cond.to(device)\n",
    "            \n",
    "            y_hat: Tensor = model(y, cond)\n",
    "            \n",
    "            for i in range(y_hat.size(0)):\n",
    "                switch, peak_reduction = cond[i, :].flatten().cpu().tolist()\n",
    "                prefix = f'{str(ii).zfill(3)}-switch={switch}-peak-reduction={peak_reduction}'\n",
    "\n",
    "                x_audio = x[i, :].flatten().cpu().numpy()\n",
    "                y_audio = y[i, :].flatten().cpu().numpy()\n",
    "                y_hat_audio = y_hat[i, :].flatten().cpu().numpy()\n",
    "                y_diff_audio = y_audio - y_hat_audio\n",
    "\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-x.wav', SignalTrainDatasetModule.sample_rate, x_audio)\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-y.wav', SignalTrainDatasetModule.sample_rate, y_audio)\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-y-hat.wav', SignalTrainDatasetModule.sample_rate, y_hat_audio)\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-y-diff.wav', SignalTrainDatasetModule.sample_rate, y_diff_audio)\n",
    "\n",
    "                ii += 1\n",
    "\n",
    "evaluate_output_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4-dynamic-range-compressor-WjUGfTKg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
