{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import freqz\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from src.dataset import SignalTrainDataset, download_signal_train_dataset_to\n",
    "from src.loss import forge_validation_criterions_by\n",
    "from src.model import S4ConditionalModel\n",
    "from src.model.layer import DSSM, convert_to_decibel\n",
    "from src.parameter import ConditionalTaskParameter\n",
    "from src.utils import get_tensor_device, set_random_seed_to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 Hyper-conditioning Dynamic Range Compressor Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains routine to evaluate a trained single-chain S4 DRC model with hyper-conditioning.\n",
    "\n",
    "Edit and execute the code in the [Preparatory Work](#preparatory-work) section first to load the model,\n",
    "and then execude the code in the rest of the section to evaluate each individual metrics.\n",
    "\n",
    "Each individual evaluation task is wrapped in a function to prevent variables going global,\n",
    "All functions will save results to the local file system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"preparatory-work\">Preparatory Work</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the following global constant to locate the model to be evaluated.\n",
    "\n",
    "If the model is trained properly using my given script, you don't need to edit any other cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path('./data/SignalTrain')\n",
    "\n",
    "CHECKPOINT_DIR = Path('./experiment-result')\n",
    "\n",
    "# Single chain with tanh\n",
    "# JOB_NAME = '2023-5-10-15-27-38'\n",
    "# EPOCH = 66\n",
    "\n",
    "# Side chain with tanh\n",
    "JOB_NAME = '2023-5-10-23-54-7'\n",
    "EPOCH = 66\n",
    "\n",
    "TESTING_DATASET_SEGMENT_LENGTH = 10.0\n",
    "TESTING_DATASET_BATCH_SIZE = 5\n",
    "TESTING_DATASET_SAMPLE_INDEX = [10, 20, 100, 200, 234]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following code to load the model and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64,\n",
      " 'checkpoint_dir': PosixPath('experiment-result'),\n",
      " 'data_segment_length': 1.0,\n",
      " 'dataset_dir': PosixPath('data/SignalTrain'),\n",
      " 'enable_learning_rate_scheduler': True,\n",
      " 'epoch': 70,\n",
      " 'learning_rate': 0.001,\n",
      " 'log_wandb': True,\n",
      " 'loss': 'ESR+DC+Multi-STFT',\n",
      " 'loss_filter_coef': 0.85,\n",
      " 'model_activation': 'PReLU',\n",
      " 'model_convert_to_decibels': False,\n",
      " 'model_depth': 4,\n",
      " 'model_film_take_batchnorm': True,\n",
      " 'model_inner_audio_channel': 32,\n",
      " 'model_s4_hidden_size': 4,\n",
      " 'model_take_residual_connection': True,\n",
      " 'model_take_side_chain': True,\n",
      " 'model_take_tanh': True,\n",
      " 'random_seed': 42,\n",
      " 's4_learning_rate': 0.001,\n",
      " 'save_checkpoint': True,\n",
      " 'wandb_entity': 'int0thewind',\n",
      " 'wandb_project_name': 'S4 Dynamic Range Compressor'}\n",
      "The SignalTrain dataset has been downloaded. Skipping ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89ff88e35f740e88ff9933d44969521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading test dataset.:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c60b61372f4fbdbe86c85ce52636b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading test dataset.:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff9259d84ae498b8f0bb2b4ce9906f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading test dataset.:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dir = CHECKPOINT_DIR / JOB_NAME\n",
    "job_eval_dir = job_dir / 'evaluations'\n",
    "job_eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = get_tensor_device(apple_silicon=False)  # Some operations are not supported on Apple Silicon\n",
    "param = ConditionalTaskParameter.from_json(CHECKPOINT_DIR / JOB_NAME / 'config.json')\n",
    "pprint(param.to_dict())\n",
    "\n",
    "set_random_seed_to(param.random_seed)\n",
    "\n",
    "download_signal_train_dataset_to(DATASET_DIR)\n",
    "testing_dataset_short = SignalTrainDataset(DATASET_DIR, 'test', 1.5)\n",
    "testing_dataset_mid = SignalTrainDataset(DATASET_DIR, 'test', 4)\n",
    "testing_dataset_long = SignalTrainDataset(DATASET_DIR, 'test', 10)\n",
    "testing_datasets = {\n",
    "    'short': testing_dataset_short,\n",
    "    'mid': testing_dataset_mid,\n",
    "    'long': testing_dataset_long,\n",
    "}\n",
    "\n",
    "model = S4ConditionalModel(\n",
    "    param.model_take_side_chain,\n",
    "    param.model_inner_audio_channel,\n",
    "    param.model_s4_hidden_size,\n",
    "    param.s4_learning_rate,\n",
    "    param.model_depth,\n",
    "    param.model_film_take_batchnorm,\n",
    "    param.model_take_residual_connection,\n",
    "    param.model_convert_to_decibels,\n",
    "    param.model_take_tanh,\n",
    "    param.model_activation,\n",
    ").eval().to(device)\n",
    "model.load_state_dict(torch.load(CHECKPOINT_DIR / JOB_NAME / f'model-epoch-{EPOCH}.pth', map_location=device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95844bb44ed4dc89730938300a4f62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing short datset.:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e405914fcafa4ce9829b162b5d73b91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing mid datset.:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0984d674c2f845db82b081bc0627bc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing long datset.:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    for dataset_name, dataset in testing_datasets.items():\n",
    "        dataloader = DataLoader(dataset, 20, num_workers=8, pin_memory=True)\n",
    "\n",
    "        validation_criterions = forge_validation_criterions_by(param.loss_filter_coef, device)\n",
    "        validation_losses = {\n",
    "            validation_loss: 0.0\n",
    "            for validation_loss in validation_criterions.keys()\n",
    "        }\n",
    "\n",
    "        for x, y, parameters in tqdm(\n",
    "            dataloader, desc=f'Testing {dataset_name} datset.', total=len(dataloader)\n",
    "        ):\n",
    "            x: Tensor = x.to(device)\n",
    "            y: Tensor = y.to(device)\n",
    "            parameters: Tensor = parameters.to(device)\n",
    "\n",
    "            y_hat: Tensor = model(x, parameters)\n",
    "\n",
    "            for validation_loss, validation_criterion in validation_criterions.items():\n",
    "                loss: Tensor = validation_criterion(y_hat.unsqueeze(1), y.unsqueeze(1))\n",
    "                validation_losses[validation_loss] += loss.item()\n",
    "        \n",
    "        for k, v in list(validation_losses.items()):\n",
    "            validation_losses[k] = v / len(dataloader)\n",
    "        \n",
    "        with open(job_eval_dir / f'loss-{dataset_name}.txt', 'w') as f:\n",
    "            pprint(validation_losses, stream=f)\n",
    "\n",
    "test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4 Frequency Response Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57565d9b2565494099f31ea86725aafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block 0.:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0563ed55f16491e91ac1a06a6f6b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block 1.:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b1f7aa6c974344b1610517cd191995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block 2.:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dab38c1b8c944e9875eb7572e6f523b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block 3.:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def s4_frequency_response_analysis():\n",
    "    out_dir = job_eval_dir / 's4-impulse-response'\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    for c, block in enumerate(model.blocks):\n",
    "        s4 = block.s4\n",
    "        assert isinstance(s4, DSSM)\n",
    "        kernel = s4.get_kernel(int(SignalTrainDataset.sample_rate * 1))\n",
    "        for r in trange(param.model_inner_audio_channel, desc=f'Block {c}.'):\n",
    "            impulse_response = kernel[r, :].detach().cpu().numpy()\n",
    "            w, h = freqz(impulse_response)\n",
    "            title = f'layer-{c + 1}-channel-{r + 1}'\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_title(title)\n",
    "            ax.plot(w, 20 * np.log10(abs(h)), 'b')\n",
    "            ax.set_xlabel('Frequency [rad/sample]')\n",
    "            ax.set_ylabel('Amplitude [dB]', color='b')\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(w, np.unwrap(np.angle(h)), 'g')\n",
    "            ax2.set_ylabel('Angle (radians)', color='g')\n",
    "            ax2.grid(True)\n",
    "        \n",
    "            fig.savefig(str(out_dir / f'{title}.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "s4_frequency_response_analysis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Inference Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NotImplemented"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_inference_efficiency():\n",
    "    return NotImplemented\n",
    "    if device.type == 'cpu':\n",
    "        print(f'Doing inference speed test on CPU...')\n",
    "    elif device.type == 'cuda':\n",
    "        print(f'Doing inference speed test on {torch.cuda.get_device_name()}.')\n",
    "    \n",
    "    print(f'Individual sample length: {TESTING_DATASET_SEGMENT_LENGTH} seconds.')\n",
    "\n",
    "    inference_time: list[int] = []\n",
    "\n",
    "    for i in tqdm(range(10)):\n",
    "        x, _, cond = testing_dataset.collate_fn([testing_dataset[i]])\n",
    "        x = x.to(device)\n",
    "        cond = cond.to(device)\n",
    "\n",
    "        tic = time.perf_counter_ns()\n",
    "        model(x, cond)\n",
    "        toc = time.perf_counter_ns()\n",
    "        inference_time.append(toc - tic)\n",
    "    \n",
    "    inference_time_mean = mean(inference_time) / 1e6\n",
    "    print(f'Average inference time: {inference_time_mean} ms.')\n",
    "    inference_time_stdev = stdev(inference_time) / 1e6\n",
    "    print(f'Inference time standard deviation: {inference_time_stdev} ms.')\n",
    "    speed_ratio = inference_time_mean / (TESTING_DATASET_SEGMENT_LENGTH * 1e3)\n",
    "    print(f'Real-time speed ratio: {speed_ratio}.')\n",
    "\n",
    "evaluate_inference_efficiency()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c4135c745a4f2a83a8868fa634575b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate long dataset.:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_output_audio():\n",
    "    # Audio output, waveform difference, RMS difference (TODO) and STFT difference\n",
    "    for dataset_name, dataset in testing_datasets.items():\n",
    "        if dataset_name != 'long':\n",
    "            continue\n",
    "\n",
    "        dataloader = DataLoader(dataset, 20, num_workers=8, pin_memory=True)\n",
    "\n",
    "        output_dir = job_eval_dir / f'output-audio-{dataset_name}'\n",
    "        output_dir.mkdir()\n",
    "\n",
    "        ii = 0\n",
    "        for x, y, cond in tqdm(dataloader, desc=f'Evaluate {dataset_name} dataset.', total=len(dataloader)):\n",
    "            x: Tensor = x.to(device)\n",
    "            y: Tensor = y.to(device)\n",
    "            cond: Tensor = cond.to(device)\n",
    "            \n",
    "            y_hat: Tensor = model(y, cond)\n",
    "            \n",
    "            for i in range(y_hat.size(0)):\n",
    "                switch, peak_reduction = cond[i, :].flatten().cpu().tolist()\n",
    "                x_audio = x[i, :].flatten()\n",
    "                y_audio = y[i, :].flatten()\n",
    "                y_hat_audio = y_hat[i, :].flatten()\n",
    "                y_diff_audio = y_audio - y_hat_audio\n",
    "\n",
    "                y_stft = torch.stft(y_audio, n_fft=1024, hop_length=256, win_length=1024, return_complex=True)\n",
    "                y_hat_stft = torch.stft(y_hat_audio, n_fft=1024, hop_length=256, win_length=1024, return_complex=True)\n",
    "                y_diff_stft = (y_stft.abs() - y_hat_stft.abs()).log10().mul(10).cpu().numpy()\n",
    "\n",
    "                x_audio = x_audio.cpu().numpy()\n",
    "                y_audio = y_audio.cpu().numpy()\n",
    "                y_hat_audio = y_hat_audio.cpu().numpy()\n",
    "                y_diff_audio = y_diff_audio.cpu().numpy()\n",
    "\n",
    "                prefix = f'{str(ii).zfill(3)}-switch={switch}-peak-reduction={peak_reduction}'\n",
    "                wavfile.write(output_dir / f'{prefix}-x.wav', SignalTrainDataset.sample_rate, x_audio)\n",
    "                wavfile.write(output_dir / f'{prefix}-y.wav', SignalTrainDataset.sample_rate, y_audio)\n",
    "                wavfile.write(output_dir / f'{prefix}-y-hat.wav', SignalTrainDataset.sample_rate, y_hat_audio)\n",
    "                wavfile.write(output_dir / f'{prefix}-y-diff.wav', SignalTrainDataset.sample_rate, y_diff_audio)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(25, 5))\n",
    "                ax.plot(y_diff_audio)\n",
    "                ax.set_title(f'{prefix}')\n",
    "                ax.set_xlabel('Time (s)')\n",
    "                ax.set_ylabel('Amplitude')\n",
    "                fig.savefig(str(output_dir / f'{prefix}-y-diff.png'))\n",
    "                plt.close(fig)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(25, 5))\n",
    "                ax.pcolormesh(y_diff_stft, cmap='jet')\n",
    "                ax.set_title(f'{prefix}')\n",
    "                ax.set_xlabel('Time (s)')\n",
    "                ax.set_ylabel('Magnitude')\n",
    "                fig.savefig(str(output_dir / f'{prefix}-y-stft-diff.png'))\n",
    "                plt.close(fig)\n",
    "\n",
    "                ii += 1\n",
    "\n",
    "evaluate_output_audio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RMS Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NotImplemented"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_rms_difference():\n",
    "    return NotImplemented\n",
    "    x, y, cond = testing_dataset.collate_fn([testing_dataset[i] for i in TESTING_DATASET_SAMPLE_INDEX])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    cond = cond.to(device)\n",
    "    \n",
    "    y_hat: Tensor = model(y, cond)\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 1, figsize=(25, 25))\n",
    "    for i, (y_audio, y_hat_audio, cond_data) in enumerate(zip(\n",
    "        y.split(1), y_hat.split(1), cond.split(1),\n",
    "    )):\n",
    "        diff = (y_audio - y_hat_audio).flatten().cpu()\n",
    "        switch, peak_reduction = cond_data.flatten().cpu().tolist()\n",
    "        ax = axes[i]\n",
    "        ax.plot(diff.numpy())\n",
    "        ax.set_title(\n",
    "            f'Waveform Difference (Switch: {switch}, '\n",
    "            f'Peak reduction: {peak_reduction}) '\n",
    "            f'Mean {diff.abs().mean().item():.8f}'\n",
    "        )\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "evaluate_rms_difference()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Step Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_step_response():\n",
    "    parameter_pair = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 5],\n",
    "        [0, 20],\n",
    "        [0, 35],\n",
    "        [0, 50],\n",
    "        [0, 65],\n",
    "        [0, 80],\n",
    "        [0, 95],\n",
    "        [0, 100],\n",
    "        [1, 0],\n",
    "        [1, 5],\n",
    "        [1, 20],\n",
    "        [1, 35],\n",
    "        [1, 50],\n",
    "        [1, 65],\n",
    "        [1, 80],\n",
    "        [1, 95],\n",
    "        [1, 100],\n",
    "        [0, 2],\n",
    "        [0, 18],\n",
    "        [0, 34],\n",
    "        [0, 53],\n",
    "        [0, 78],\n",
    "        [0, 97],\n",
    "        [1, 2],\n",
    "        [1, 18],\n",
    "        [1, 34],\n",
    "        [1, 53],\n",
    "        [1, 78],\n",
    "        [1, 97],\n",
    "    ]).to(device, torch.float32)\n",
    "    \n",
    "    sr = SignalTrainDataset.sample_rate\n",
    "    step_signal = torch.cat([\n",
    "        torch.zeros(int(sr * 0.2)),\n",
    "        torch.ones(int(sr * 0.8)),\n",
    "        torch.zeros(int(sr * 0.8)) + 0.2,\n",
    "        torch.ones(int(sr * 0.8)),\n",
    "        torch.zeros(int(sr * 0.2)),\n",
    "    ]).to(device, torch.float32)\n",
    "    output_signals: Tensor = model(\n",
    "        step_signal.repeat(parameter_pair.size(0), 1),\n",
    "        parameter_pair,\n",
    "    )\n",
    "    step_signal_decibel = convert_to_decibel(step_signal)\n",
    "    \n",
    "    fig, axs = plt.subplots(10, 3, figsize=(15, 50))\n",
    "    for i, output_signal in enumerate(output_signals.split(1)):\n",
    "        switch, peak_reduction = parameter_pair[i].tolist()\n",
    "        row, col = divmod(i, 3)\n",
    "        ax = axs[row][col]\n",
    "        ax.plot(step_signal_decibel.cpu().numpy(), color='blue', linestyle='dashed', alpha=1.0)\n",
    "        ax.plot(convert_to_decibel(output_signal).flatten().cpu().numpy(), color='red', linestyle='solid', alpha=0.5)\n",
    "        ax.set_title(f'{switch = }, {peak_reduction = }')\n",
    "    fig.savefig(str(job_eval_dir / f'model-step-response.png'))\n",
    "    plt.close(fig)\n",
    "    \n",
    "evaluate_model_step_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4-dynamic-range-compressor-WjUGfTKg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
