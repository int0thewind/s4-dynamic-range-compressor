{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import freqz\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from src.dataset import SignalTrainDataset, download_signal_train_dataset_to\n",
    "from src.loss import forge_validation_criterions_by\n",
    "from src.model import S4ConditionalModel\n",
    "from src.model.layer import DSSM, convert_to_decibel\n",
    "from src.parameter import ConditionalTaskParameter\n",
    "from src.utils import get_tensor_device, set_random_seed_to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 Hyper-conditioning Dynamic Range Compressor Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains routine to evaluate a trained single-chain S4 DRC model with hyper-conditioning.\n",
    "\n",
    "Edit and execute the code in the [Preparatory Work](#preparatory-work) section first to load the model,\n",
    "and then execude the code in the rest of the section to evaluate each individual metrics.\n",
    "\n",
    "Each individual evaluation task is wrapped in a function to prevent variables going global,\n",
    "All functions will save results to the local file system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"preparatory-work\">Preparatory Work</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the following global constant to locate the model to be evaluated.\n",
    "\n",
    "If the model is trained properly using my given script, you don't need to edit any other cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path('./data/SignalTrain')\n",
    "CHECKPOINT_DIR = Path('./experiment-result')\n",
    "\n",
    "# Single chain without tanh\n",
    "# JOB_NAME = '2023-5-9-22-34-47'\n",
    "# JOB_NAME = 'single-chain-no-tanh'\n",
    "# EPOCH = 54\n",
    "\n",
    "# Side chain without tanh\n",
    "# JOB_NAME = '2023-5-10-7-1-39'\n",
    "# JOB_NAME = 'side-chain-no-tanh'\n",
    "# EPOCH = 68\n",
    "\n",
    "# Single chain with tanh\n",
    "# JOB_NAME = '2023-5-10-15-27-38'\n",
    "# JOB_NAME = 'single-chain-tanh'\n",
    "# EPOCH = 66\n",
    "\n",
    "# Side chain with tanh\n",
    "# JOB_NAME = '2023-5-10-23-54-7'\n",
    "# JOB_NAME = 'side-chain-tanh'\n",
    "# EPOCH = 66\n",
    "\n",
    "# Single chain with parametered tanh\n",
    "# JOB_NAME = '2023-6-28-3-32-47'\n",
    "# JOB_NAME = 'single-chain-ptanh'\n",
    "# EPOCH = 52\n",
    "\n",
    "# Side chain with parametered tanh\n",
    "# JOB_NAME = '2023-6-28-16-11-36'\n",
    "JOB_NAME = 'side-chain-ptanh'\n",
    "EPOCH = 40"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following code to load the model and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = CHECKPOINT_DIR / JOB_NAME\n",
    "job_eval_dir = job_dir / 'evaluations'\n",
    "job_eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = get_tensor_device(apple_silicon=False)  # Some operations are not supported on Apple Silicon\n",
    "param = ConditionalTaskParameter.from_json(CHECKPOINT_DIR / JOB_NAME / 'config.json')\n",
    "pprint(param.to_dict())\n",
    "\n",
    "set_random_seed_to(param.random_seed)\n",
    "\n",
    "download_signal_train_dataset_to(DATASET_DIR)\n",
    "testing_dataset_short = SignalTrainDataset(DATASET_DIR, 'test', 1.5)\n",
    "testing_dataset_mid = SignalTrainDataset(DATASET_DIR, 'test', 4)\n",
    "testing_dataset_long = SignalTrainDataset(DATASET_DIR, 'test', 10)\n",
    "testing_datasets = {\n",
    "    'short': testing_dataset_short,\n",
    "    'mid': testing_dataset_mid,\n",
    "    'long': testing_dataset_long,\n",
    "}\n",
    "testing_dataset_lengths = {\n",
    "    'short': 1.5,\n",
    "    'mid': 4,\n",
    "    'long': 10,\n",
    "}\n",
    "\n",
    "model = S4ConditionalModel(\n",
    "    param.model_take_side_chain,\n",
    "    param.model_inner_audio_channel,\n",
    "    param.model_s4_hidden_size,\n",
    "    param.s4_learning_rate,\n",
    "    param.model_depth,\n",
    "    param.model_film_take_batchnorm,\n",
    "    param.model_take_residual_connection,\n",
    "    param.model_convert_to_decibels,\n",
    "    param.model_take_tanh,\n",
    "    param.model_activation,\n",
    "    param.model_take_parametered_tanh,\n",
    ").eval().to(device)\n",
    "model.load_state_dict(torch.load(CHECKPOINT_DIR / JOB_NAME / f'model-epoch-{EPOCH}.pth', map_location=device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    for dataset_name, dataset in testing_datasets.items():\n",
    "        dataloader = DataLoader(dataset, 20, num_workers=8, pin_memory=True)\n",
    "\n",
    "        validation_criterions = forge_validation_criterions_by(param.loss_filter_coef, device)\n",
    "        validation_losses = {\n",
    "            validation_loss: 0.0\n",
    "            for validation_loss in validation_criterions.keys()\n",
    "        }\n",
    "\n",
    "        for x, y, parameters in tqdm(\n",
    "            dataloader, desc=f'Testing {dataset_name} datset.', total=len(dataloader)\n",
    "        ):\n",
    "            x: Tensor = x.to(device)\n",
    "            y: Tensor = y.to(device)\n",
    "            parameters: Tensor = parameters.to(device)\n",
    "\n",
    "            y_hat: Tensor = model(x, parameters)\n",
    "\n",
    "            for validation_loss, validation_criterion in validation_criterions.items():\n",
    "                loss: Tensor = validation_criterion(y_hat.unsqueeze(1), y.unsqueeze(1))\n",
    "                validation_losses[validation_loss] += loss.item()\n",
    "        \n",
    "        for k, v in list(validation_losses.items()):\n",
    "            validation_losses[k] = v / len(dataloader)\n",
    "        \n",
    "        with open(job_eval_dir / f'loss-{dataset_name}.txt', 'w') as f:\n",
    "            pprint(validation_losses, stream=f)\n",
    "\n",
    "test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4 Frequency Response Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def s4_frequency_response_analysis():\n",
    "    out_dir = job_eval_dir / 's4-impulse-response'\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    for c, block in enumerate(model.blocks):\n",
    "        s4 = block.s4\n",
    "        assert isinstance(s4, DSSM)\n",
    "        kernel = s4.get_kernel(int(SignalTrainDataset.sample_rate * 1))\n",
    "        for r in trange(param.model_inner_audio_channel, desc=f'Block {c}.'):\n",
    "            impulse_response = kernel[r, :].detach().cpu().numpy()\n",
    "            w, h = freqz(impulse_response)\n",
    "            title = f'layer-{c + 1}-channel-{r + 1}'\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_title(title)\n",
    "            ax.plot(w, 20 * np.log10(abs(h)), 'b')\n",
    "            ax.set_xlabel('Frequency [rad/sample]')\n",
    "            ax.set_ylabel('Amplitude [dB]', color='b')\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(w, np.unwrap(np.angle(h)), 'g')\n",
    "            ax2.set_ylabel('Angle (radians)', color='g')\n",
    "            ax2.grid(True)\n",
    "        \n",
    "            fig.savefig(str(out_dir / f'{title}.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "s4_frequency_response_analysis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Inference Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_inference_efficiency():\n",
    "    if device.type == 'cpu':\n",
    "        print(f'Doing inference speed test on CPU...')\n",
    "        device_name = 'cpu'\n",
    "    elif device.type == 'cuda':\n",
    "        print(f'Doing inference speed test on {(device_name := torch.cuda.get_device_name())}.')\n",
    "    else:\n",
    "        raise NotImplementedError(f'Inference efficiency test can only run on CPU/CUDA')\n",
    "\n",
    "    for dataset_name, testing_dataset in testing_datasets.items():\n",
    "        dataset_sample_length = testing_dataset_lengths[dataset_name]\n",
    "\n",
    "        inference_time: list[int] = []\n",
    "        for i in tqdm(range(10)):\n",
    "            x, _, cond = testing_dataset[i]\n",
    "            x = x.to(device).unsqueeze(0)\n",
    "            cond = cond.to(device).unsqueeze(0)\n",
    "\n",
    "            tic = time.perf_counter_ns()\n",
    "            model(x, cond)\n",
    "            toc = time.perf_counter_ns()\n",
    "            inference_time.append(toc - tic)\n",
    "        \n",
    "        inference_time_mean = mean(inference_time) / 1e6\n",
    "        inference_time_stdev = stdev(inference_time) / 1e6\n",
    "        speed_ratio = inference_time_mean / (dataset_sample_length * 1e3)\n",
    "\n",
    "        with open(job_eval_dir / f'inference-efficiency-{dataset_name}-{device_name}.txt', 'w') as f:\n",
    "            print(f'Average inference time on {dataset_name} dataset: {inference_time_mean} ms. ', file=f)\n",
    "            print(f'Inference time standard deviation on {dataset_name} dataset: {inference_time_stdev} ms. ', file=f)\n",
    "            print(f'Real-time speed ratio on {dataset_name} dataset: {speed_ratio}. ', file=f)\n",
    "\n",
    "evaluate_inference_efficiency()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Output Audio, Waveform Difference, RMS Difference, and STFT Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_rms(x: npt.NDArray[np.float32], window: int = 100):\n",
    "    assert x.ndim == 1\n",
    "    size = x.size\n",
    "    ret = np.fromiter(\n",
    "        (np.sqrt(np.mean(np.square(x[i:i + window]))) for i in range(0, size, window)),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_output_audio():\n",
    "    # Audio output, waveform difference, RMS difference and STFT difference\n",
    "    for dataset_name, dataset in testing_datasets.items():\n",
    "        if dataset_name != 'long':\n",
    "            continue\n",
    "\n",
    "        dataloader = DataLoader(dataset, 20, num_workers=8, pin_memory=True)\n",
    "\n",
    "        output_audio_dir = job_eval_dir / f'output-audio-{dataset_name}'\n",
    "        output_audio_dir.mkdir(exist_ok=True)\n",
    "        output_rms_dir = job_eval_dir / f'output-rms-{dataset_name}'\n",
    "        output_rms_dir.mkdir(exist_ok=True)\n",
    "        output_waveform_dir = job_eval_dir / f'output-waveform-{dataset_name}'\n",
    "        output_waveform_dir.mkdir(exist_ok=True)\n",
    "        output_stft_dir = job_eval_dir / f'output-stft-{dataset_name}'\n",
    "        output_stft_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        ii = 0\n",
    "        for x, y, cond in tqdm(dataloader, desc=f'Evaluate {dataset_name} dataset.', total=len(dataloader)):\n",
    "            x: Tensor = x.to(device)\n",
    "            y: Tensor = y.to(device)\n",
    "            cond: Tensor = cond.to(device)\n",
    "            \n",
    "            y_hat: Tensor = model(y, cond)\n",
    "            \n",
    "            for i in range(y_hat.size(0)):\n",
    "                switch, peak_reduction = cond[i, :].flatten().cpu().tolist()\n",
    "                prefix = f'{str(ii).zfill(3)}-switch={switch}-peak-reduction={peak_reduction}'\n",
    "    \n",
    "                x_audio = x[i, :].flatten()\n",
    "                y_audio = y[i, :].flatten()\n",
    "                y_hat_audio = y_hat[i, :].flatten()\n",
    "                y_diff_audio = y_audio - y_hat_audio\n",
    "\n",
    "                y_stft = torch.stft(y_audio, n_fft=1024, hop_length=256, win_length=1024, return_complex=True)\n",
    "                y_hat_stft = torch.stft(y_hat_audio, n_fft=1024, hop_length=256, win_length=1024, return_complex=True)\n",
    "                y_diff_stft = (y_stft.abs() - y_hat_stft.abs()).log10().mul(10).cpu().numpy()\n",
    "\n",
    "                x_audio = x_audio.cpu().numpy()\n",
    "                y_audio = y_audio.cpu().numpy()\n",
    "                y_hat_audio = y_hat_audio.cpu().numpy()\n",
    "                y_diff_audio = y_diff_audio.cpu().numpy()\n",
    "\n",
    "                y_rms = acquire_rms(y_audio)\n",
    "                y_hat_rms = acquire_rms(y_hat_audio)\n",
    "                y_diff_rms = y_rms - y_hat_rms\n",
    "\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-x.wav', SignalTrainDataset.sample_rate, x_audio)\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-y.wav', SignalTrainDataset.sample_rate, y_audio)\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-y-hat.wav', SignalTrainDataset.sample_rate, y_hat_audio)\n",
    "                wavfile.write(output_audio_dir / f'{prefix}-y-diff.wav', SignalTrainDataset.sample_rate, y_diff_audio)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(25, 5))\n",
    "                ax.plot(y_diff_audio)\n",
    "                ax.set_title(f'{prefix}')\n",
    "                ax.set_xlabel('Time (s)')\n",
    "                ax.set_ylabel('Amplitude')\n",
    "                fig.savefig(str(output_waveform_dir / f'{prefix}-y-diff.png'))\n",
    "                plt.close(fig)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(25, 5))\n",
    "                ax.plot(y_diff_rms)\n",
    "                ax.set_title(f'{prefix}')\n",
    "                ax.set_xlabel('Time (s)')\n",
    "                ax.set_ylabel('Amplitude')\n",
    "                fig.savefig(str(output_rms_dir / f'{prefix}-y-rms-diff.png'))\n",
    "                plt.close(fig)\n",
    "\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(25, 5))\n",
    "                ax.pcolormesh(y_diff_stft, cmap='jet')\n",
    "                ax.set_title(f'{prefix}')\n",
    "                ax.set_xlabel('Time (s)')\n",
    "                ax.set_ylabel('Magnitude')\n",
    "                fig.savefig(str(output_stft_dir / f'{prefix}-y-stft-diff.png'))\n",
    "                plt.close(fig)\n",
    "\n",
    "                ii += 1\n",
    "\n",
    "evaluate_output_audio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Step Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_step_response():\n",
    "    parameter_pair = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 5],\n",
    "        [0, 20],\n",
    "        [0, 35],\n",
    "        [0, 50],\n",
    "        [0, 65],\n",
    "        [0, 80],\n",
    "        [0, 95],\n",
    "        [0, 100],\n",
    "        [1, 0],\n",
    "        [1, 5],\n",
    "        [1, 20],\n",
    "        [1, 35],\n",
    "        [1, 50],\n",
    "        [1, 65],\n",
    "        [1, 80],\n",
    "        [1, 95],\n",
    "        [1, 100],\n",
    "        [0, 2],\n",
    "        [0, 18],\n",
    "        [0, 34],\n",
    "        [0, 53],\n",
    "        [0, 78],\n",
    "        [0, 97],\n",
    "        [1, 2],\n",
    "        [1, 18],\n",
    "        [1, 34],\n",
    "        [1, 53],\n",
    "        [1, 78],\n",
    "        [1, 97],\n",
    "    ]).to(device, torch.float32)\n",
    "    \n",
    "    sr = SignalTrainDataset.sample_rate\n",
    "    step_signal = torch.cat([\n",
    "        torch.zeros(int(sr * 0.2)),\n",
    "        torch.ones(int(sr * 0.8)),\n",
    "        torch.zeros(int(sr * 0.8)) + 0.2,\n",
    "        torch.ones(int(sr * 0.8)),\n",
    "        torch.zeros(int(sr * 0.2)),\n",
    "    ]).to(device, torch.float32)\n",
    "    output_signals: Tensor = model(\n",
    "        step_signal.repeat(parameter_pair.size(0), 1),\n",
    "        parameter_pair,\n",
    "    )\n",
    "    step_signal_decibel = convert_to_decibel(step_signal)\n",
    "    \n",
    "    fig, axs = plt.subplots(10, 3, figsize=(15, 50))\n",
    "    for i, output_signal in enumerate(output_signals.split(1)):\n",
    "        switch, peak_reduction = parameter_pair[i].tolist()\n",
    "        row, col = divmod(i, 3)\n",
    "        ax = axs[row][col]\n",
    "        ax.plot(step_signal_decibel.cpu().numpy(), color='blue', linestyle='dashed', alpha=1.0)\n",
    "        ax.plot(convert_to_decibel(output_signal).flatten().cpu().numpy(), color='red', linestyle='solid', alpha=0.5)\n",
    "        ax.set_title(f'{switch = }, {peak_reduction = }')\n",
    "    fig.savefig(str(job_eval_dir / f'model-step-response.png'))\n",
    "    plt.close(fig)\n",
    "    \n",
    "evaluate_model_step_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4-dynamic-range-compressor-WjUGfTKg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
