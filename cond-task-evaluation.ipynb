{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from IPython.display import Audio, display\n",
    "from torch import Tensor\n",
    "\n",
    "from src.dataset import SignalTrainDataset, download_signal_train_dataset_to\n",
    "from src.model import S4ConditionalSideChainModel\n",
    "from src.model.layer import convert_to_decibel\n",
    "from src.parameter import ConditionalTaskParameter\n",
    "from src.utils import get_tensor_device, set_random_seed_to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 Hyper-conditioning Dynamic Range Compressor Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains routine to evaluate a trained S4 DRC model with hyper-conditioning.\n",
    "\n",
    "Edit and execute the code in the [Preparatory Work](#preparatory-work) section first to load the model,\n",
    "and then execude the code in the rest of the section to evaluate each individual metrics.\n",
    "\n",
    "Each individual evaluation task is wrapped in a function to prevent variables going global,\n",
    "and neither any function alters the file system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"preparatory-work\">Preparatory Work</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the following global constant to locate the model to be evaluated.\n",
    "\n",
    "If the model is trained properly using my given script, you don't need to edit any other cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path('./data/SignalTrain')\n",
    "\n",
    "CHECKPOINT_DIR = Path('./experiment-result')\n",
    "JOB_NAME = '2023-4-7-16-35-36'\n",
    "EPOCH = '35'\n",
    "\n",
    "TESTING_DATASET_SEGMENT_LENGTH = 10.0\n",
    "TESTING_DATASET_BATCH_SIZE = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following code to load the model and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = CHECKPOINT_DIR / JOB_NAME\n",
    "device = get_tensor_device(apple_silicon=False)  # Some operations are not supported on Apple Silicon\n",
    "param = ConditionalTaskParameter.from_json(CHECKPOINT_DIR / JOB_NAME / 'config.json')\n",
    "\n",
    "set_random_seed_to(param.random_seed)\n",
    "\n",
    "download_signal_train_dataset_to(DATASET_DIR)\n",
    "testing_dataset = SignalTrainDataset(DATASET_DIR, 'test', TESTING_DATASET_SEGMENT_LENGTH)\n",
    "\n",
    "model = S4ConditionalSideChainModel(\n",
    "    param.model_version,\n",
    "    param.model_control_parameter_mlp_depth,\n",
    "    param.model_control_parameter_mlp_hidden_size,\n",
    "    param.model_inner_audio_channel,\n",
    "    param.model_s4_hidden_size,\n",
    "    param.s4_learning_rate,\n",
    "    param.model_depth,\n",
    "    param.model_activation,\n",
    "    param.model_convert_to_decibels,\n",
    ").eval().to(device)\n",
    "model.load_state_dict(torch.load(CHECKPOINT_DIR / JOB_NAME / f'model-epoch-{EPOCH}.pth', map_location=device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_output_audio():\n",
    "    x, y, cond = testing_dataset.collate_fn([\n",
    "        testing_dataset[10],\n",
    "        testing_dataset[20],\n",
    "        testing_dataset[100],\n",
    "        testing_dataset[200],\n",
    "        testing_dataset[234],\n",
    "    ])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    cond = cond.to(device)\n",
    "    \n",
    "    y_hat: Tensor = model(y, cond)\n",
    "    \n",
    "    for x_audio, y_audio, y_hat_audio, cond_data in zip(\n",
    "        x.split(1), y.split(1), y_hat.split(1), cond.split(1),\n",
    "    ):\n",
    "        switch, peak_reduction = cond_data.flatten().cpu().tolist()\n",
    "        display(f'Switch value: {switch}, Peak reduction value: {peak_reduction}')\n",
    "        display(Audio(x_audio.flatten().cpu().numpy(), rate=testing_dataset.sample_rate))\n",
    "        display(Audio(y_audio.flatten().cpu().numpy(), rate=testing_dataset.sample_rate))\n",
    "        display(Audio(y_hat_audio.flatten().cpu().numpy(), rate=testing_dataset.sample_rate))\n",
    "\n",
    "evaluate_output_audio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Waveform Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_waveform_difference():\n",
    "    x, y, cond = testing_dataset.collate_fn([\n",
    "        testing_dataset[10],\n",
    "        testing_dataset[20],\n",
    "        testing_dataset[100],\n",
    "        testing_dataset[200],\n",
    "        testing_dataset[234],\n",
    "    ])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    cond = cond.to(device)\n",
    "    \n",
    "    y_hat: Tensor = model(y, cond)\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 1, figsize=(25, 25))\n",
    "    for i, (y_audio, y_hat_audio, cond_data) in enumerate(zip(\n",
    "        y.split(1), y_hat.split(1), cond.split(1),\n",
    "    )):\n",
    "        diff = (y_audio - y_hat_audio).flatten().cpu()\n",
    "        switch, peak_reduction = cond_data.flatten().cpu().tolist()\n",
    "        ax = axes[i]\n",
    "        ax.plot(diff.numpy())\n",
    "        ax.set_title(\n",
    "            f'Waveform Difference (Switch: {switch}, '\n",
    "            f'Peak reduction: {peak_reduction}) '\n",
    "            f'Mean {diff.abs().mean().item():.6f}'\n",
    "        )\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "evaluate_waveform_difference()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RMS Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_rms_difference():\n",
    "    x, y, cond = testing_dataset.collate_fn([\n",
    "        testing_dataset[10],\n",
    "        testing_dataset[20],\n",
    "        testing_dataset[100],\n",
    "        testing_dataset[200],\n",
    "        testing_dataset[234],\n",
    "    ])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    cond = cond.to(device)\n",
    "    \n",
    "    y_hat: Tensor = model(y, cond)\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 1, figsize=(25, 25))\n",
    "    for i, (y_audio, y_hat_audio, cond_data) in enumerate(zip(\n",
    "        y.split(1), y_hat.split(1), cond.split(1),\n",
    "    )):\n",
    "        diff = (y_audio - y_hat_audio).flatten().cpu()\n",
    "        switch, peak_reduction = cond_data.flatten().cpu().tolist()\n",
    "        ax = axes[i]\n",
    "        ax.plot(diff.numpy())\n",
    "        ax.set_title(\n",
    "            f'Waveform Difference (Switch: {switch}, '\n",
    "            f'Peak reduction: {peak_reduction}) '\n",
    "            f'Mean {diff.abs().mean().item():.8f}'\n",
    "        )\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "evaluate_rms_difference()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Step Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_step_response():\n",
    "    parameter_pair = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 5],\n",
    "        [0, 20],\n",
    "        [0, 35],\n",
    "        [0, 50],\n",
    "        [0, 65],\n",
    "        [0, 80],\n",
    "        [0, 95],\n",
    "        [0, 100],\n",
    "        [1, 0],\n",
    "        [1, 5],\n",
    "        [1, 20],\n",
    "        [1, 35],\n",
    "        [1, 50],\n",
    "        [1, 65],\n",
    "        [1, 80],\n",
    "        [1, 95],\n",
    "        [1, 100],\n",
    "        [0, 2],\n",
    "        [0, 18],\n",
    "        [0, 34],\n",
    "        [0, 53],\n",
    "        [0, 78],\n",
    "        [0, 97],\n",
    "        [1, 2],\n",
    "        [1, 18],\n",
    "        [1, 34],\n",
    "        [1, 53],\n",
    "        [1, 78],\n",
    "        [1, 97],\n",
    "    ]).to(device, torch.float32)\n",
    "    \n",
    "    sr = testing_dataset.sample_rate\n",
    "    step_signal = torch.cat([\n",
    "        torch.zeros(int(sr * 0.2)),\n",
    "        torch.ones(int(sr * 0.8)),\n",
    "        torch.zeros(int(sr * 0.8)) + 0.2,\n",
    "        torch.zeros(int(sr * 0.2)),\n",
    "    ]).to(device, torch.float32)\n",
    "    output_signals: Tensor = model(\n",
    "        step_signal.repeat(parameter_pair.size(0), 1),\n",
    "        parameter_pair,\n",
    "    )\n",
    "    step_signal_decibel = convert_to_decibel(step_signal)\n",
    "    \n",
    "    fig, axs = plt.subplots(10, 3, figsize=(15, 50))\n",
    "    for i, output_signal in enumerate(output_signals.split(1)):\n",
    "        switch, peak_reduction = parameter_pair[i].tolist()\n",
    "        row, col = divmod(i, 3)\n",
    "        ax = axs[row][col]\n",
    "        ax.plot(step_signal_decibel.cpu().numpy(), color='blue', linestyle='dashed', alpha=1.0)\n",
    "        ax.plot(convert_to_decibel(output_signal).flatten().cpu().numpy(), color='red', linestyle='solid', alpha=0.5)\n",
    "        ax.set_title(f'{switch = }, {peak_reduction = }')\n",
    "    fig.show()\n",
    "    \n",
    "evaluate_model_step_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4-dynamic-range-compressor-WjUGfTKg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
